# -*- coding: utf-8 -*-
"""Contextualized_Embedding.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1slL6AkC9kz4X_cC_VkKaAR7YkJ64rmMI
"""

from transformers import BertModel, BertConfig, BertTokenizer
import numpy as np
import torch
class Contextualized_Embedding:
  def __init__(self, model, tokenizer):
        # 初始化定义模型和分词器
        self.model = model
        self.tokenizer = tokenizer
  def contextualizedEmbedding(self, text):
    # Add the special tokens.
    marked_text = "[CLS] " + text + " [SEP]"

    # Split the sentence into tokens.
    tokenized_text = self.tokenizer.tokenize(marked_text)

    # Map the token strings to their vocabulary indeces.
    indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_text)

    # Mark each of the 22 tokens as belonging to sentence "1".
    segments_ids = [1] * len(tokenized_text)

    # Convert inputs to PyTorch tensors
    tokens_tensor = torch.tensor([indexed_tokens])
    segments_tensors = torch.tensor([segments_ids])

    # Put the model in "evaluation" mode, meaning feed-forward operation.
    self.Eval()

    with torch.no_grad():
      output = self.model(tokens_tensor,segments_tensors)
    encoded_layers = output[2]

    # Concatenate the tensors for all layers. We use `stack` here to
    # create a new dimension in the tensor.
    token_embeddings = torch.stack(encoded_layers, dim=0)

    # Remove dimension 1, the "batches".
    #token_embeddings = torch.squeeze(token_embeddings, dim=1)

    # Swap dimensions 0 and 1.
    token_embeddings = token_embeddings.permute(2,1,0,3)

    # Stores the token vectors, with shape [22 x 768]
    token_vecs_sum = []

    # `token_embeddings` is a [22 x 12 x 768] tensor.

    # For each token in the sentence...
    for token in token_embeddings:

      # `token` is a [12 x 768] tensor

      # Sum the vectors from the last four layers.
      sum_vec = torch.sum(token[:,-4:], dim=1)
    
      # Use `sum_vec` to represent `token`.
      token_vecs_sum.append(sum_vec.numpy())
    tvs = np.array(token_vecs_sum)
    tvs1 = tvs.swapaxes(1,0)
    return tvs1
  def Eval(self):
    self.model.eval();
